# Rainbow

## 1. Raibowã®å…¨ä½“åƒ

![Rainbowã®æ€§èƒ½](img/rainbow/performance.png)

> Rainbowï¼ˆå›³ä¸­è™¹è‰²ï¼‰ã¯DQNã«6ã¤ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã¦å½“æ™‚ã®SoTAã‚’é”æˆï¼ˆ[[Hessel, M. 18]](https://arxiv.org/abs/1710.02298) Figure.1ï¼‰

Raibowã¯Deep Q-Networkã€é€šç§°DQNã‚’ãƒ™ãƒ¼ã‚¹ã¨ã—ãŸãƒ¢ãƒ‡ãƒ«ã§ã™ã€‚Rainbowã§ã¯ã€DQNã«6ã¤ã®æ‰‹æ³•ã‚’çµ„ã¿åˆã‚ã›ã‚‹ã“ã¨ã§ã€å¼·åŒ–å­¦ç¿’ã®ã‚¿ã‚¹ã‚¯ã§å½“æ™‚ã®æœ€é«˜æ€§èƒ½ï¼ˆSoTAï¼‰ã‚’é”æˆã—ã¾ã—ãŸã€‚æœ¬è¨˜äº‹ã§ã¯ã€DQNã«ã¤ã„ã¦ç°¡å˜ã«èª¬æ˜ã—ãŸå¾Œã€Rainbowã§ç”¨ã„ã‚‰ã‚Œã¦ã„ã‚‹æ‰‹æ³•ã«ã¤ã„ã¦èª¬æ˜ã—ã¾ã™ã€‚

* DQN
* 6ã¤ã®æ‰‹æ³•
    * Double DQN
    * å„ªå…ˆåº¦ä»˜ãçµŒé¨“ãƒãƒƒãƒ•ã‚¡
    * Dueling Network
    * Noisy Network
    * Categorical Network
    * Multi-step Learning

ã¾ãŸã€æœ¬è¨˜äº‹ã§å–ã‚Šæ‰±ã†ã‚¿ã‚¹ã‚¯ã¯ã€ä»¥ä¸‹ã®CartPoleã§ã™ã€‚

![CartPole](https://miro.medium.com/max/1100/0*-ipKnKpwgmtiDKfr.gif)

> Cart-Pole ï¼ˆ[Building a DQN in PyTorch: Balancing Cart Pole with Deep RL](https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435)ã‚ˆã‚Šå¼•ç”¨ï¼‰

ã“ã®è¨˜äº‹ã§ã¯ã€Rainbow[[Hessel, M. 18]](https://arxiv.org/abs/1710.02298)ã«ã¤ã„ã¦è§£èª¬ã—ã¦ã„ãã¾ã™ã€‚ã‚¿ã‚¹ã‚¯ã¯ã€ä¸Šã®å›³ã«ã‚ã‚‹ã‚ˆã†ãªCart-poleã‚’å–ã‚Šæ‰±ã„ã¾ã™ã€‚**Cart-poleã§ã¯ã€æ£’ã®ä¹—ã£ãŸå°è»Šã‚’å·¦å³ã«æºã‚‰ã™ã“ã¨ã§æ£’ã‚’ãªã‚‹ã¹ãé•·ã„æ™‚é–“ç«‹ãŸã›ã‚‹ã“ã¨ãŒç›®çš„**ã«ãªã‚Šã¾ã™ã€‚çŠ¶æ…‹ï¼ˆStateï¼‰ã¯ã€ã‚«ãƒ¼ãƒˆä½ç½®ãƒ»ã‚«ãƒ¼ãƒˆã®é€Ÿåº¦ãƒ»æ£’ã®è§’åº¦ãƒ»æ£’ã®è§’é€Ÿåº¦ã®4ã¤ã§ã™ã€‚è¡Œå‹•ï¼ˆActionï¼‰ã¯ã€å°è»Šã‚’å·¦ã«å‹•ã‹ã™ãƒ»å³ã«å‹•ã‹ã™ã®2ã¤ã ã‘ã§ã™ã€‚æ£’ãŒç«‹ã£ã¦ã„ã‚Œã°ã€å³æ™‚å ±é…¬ï¼ˆRewardï¼‰ã¨ã—ã¦1ãŒä¸ãˆã‚‰ã‚Œã¾ã™ã€‚


## 2. DQN

* ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«ã‚ˆã‚‹Qå­¦ç¿’
* çµŒé¨“ãƒãƒƒãƒ•ã‚¡ï¼ˆExperience Bufferï¼‰
* ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯
* å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°


### 2.1 ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«ã‚ˆã‚‹Qå­¦ç¿’

![DQNã®ä¾‹](https://miro.medium.com/max/1394/1*w8XwcjOPMe7Mw0PQg_fu1A.webp)

> DQN ï¼ˆ[Building a DQN in PyTorch: Balancing Cart Pole with Deep RL](https://blog.gofynd.com/building-a-deep-q-network-in-pytorch-fa1086aa5435)ã‚ˆã‚Šå¼•ç”¨ï¼‰

ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«ã‚ˆã‚‹Qå­¦ç¿’ã§ã¯ã€è¡Œå‹•ä¾¡å€¤ã¨å‘¼ã°ã‚Œã‚‹Qå€¤ã‚’ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã«äºˆæ¸¬ã•ã›ã¾ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã¸ã®å…¥åŠ›çŠ¶æ…‹ã€å‡ºåŠ›ã¯å„è¡Œå‹•ã«å¯¾ã™ã‚‹è¡Œå‹•ä¾¡å€¤ã¨ãªã‚Šã¾ã™ã€‚æ™‚åˆ»$t$ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’$\theta_t$ã¨ã™ã‚‹ã¨ã€$Q(s_t,a_t;\theta_t)$ãŒãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å‡ºåŠ›ã¨ãªã‚Šã¾ã™ã€‚ã“ã‚Œã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯1ã¤æ™‚åˆ»ã‚’é€²ã‚ãŸã¨ãã®è¡Œå‹•ä¾¡å€¤$r_t+\gamma \max_{a'} Q(s_{t+1}, a';\theta_t)$ã«ãªã‚Šã¾ã™ã€‚ã“ã®ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯**TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆ**ï¼ˆTD: Temporal Differenceï¼‰ã¨å‘¼ã°ã‚Œã¾ã™ã€‚ã“ã®TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯ã€$Q(s_t,a_t;\theta_t)$ã®æ™‚åˆ»ã‚’æ—¢çŸ¥ã®$r_t$ã‚’ä½¿ã£ã¦1ã¤å±•é–‹ã—ãŸã‚‚ã®ã«ãªã‚Šã¾ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’ã«ç”¨ã„ã‚‹èª¤å·®ã¯ã€ã¨TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®äºŒä¹—èª¤å·®ã§ã‚ã‚Šã€ä»¥ä¸‹ã®å¼ã¨ãªã‚Šã¾ã™ã€‚ã“ã®èª¤å·®ã¯**TDèª¤å·®**ã¨å‘¼ã°ã‚Œã¾ã™ã€‚

$$
L(\theta_t) = \mathbb{E}_{(s_t,a_t,r_t,s_{t+1})} \big[ \big( r_t + \gamma \max_{a'} Q(s_{t+1},a';\theta_t) - Q(s_t, a_t; \theta_t) \big)^2 \big]
$$

### 2.2 çµŒé¨“å†ç”Ÿï¼ˆExperience Replayï¼‰

DQNã‚’ãã®ã¾ã¾ãƒŸãƒ‹ãƒãƒƒãƒã§å­¦ç¿’ã•ã›ã‚ˆã†ã¨ã™ã‚‹ã¨ã€ãƒŸãƒ‹ãƒãƒƒãƒå†…ã®é·ç§»åŒå£«ãŒã‹ãªã‚Šç›¸é–¢ã—åˆã£ã¦ã„ã¾ã™ï¼ˆä¾‹ãˆã°ã€ãƒŸãƒ‹ãƒãƒƒãƒå†…ã«é€£ç¶šã—ãŸé·ç§»ãŒã‚ã‚‹ã¨ç›¸é–¢ãŒå¼·ãã†ãªã“ã¨ãŒã‚ã‹ã‚Šã¾ã™ï¼‰ã€‚ã“ã®ã€ŒãƒŸãƒ‹ãƒãƒƒãƒå†…ã®ç›¸é–¢ã€ã‚’æ¶ˆã™ãŸã‚ã«å°å…¥ã•ã‚ŒãŸã‚‚ã®ãŒçµŒé¨“ãƒãƒƒãƒ•ã‚¡ã§ã™ã€‚

ä»•çµ„ã¿ã¯éå¸¸ã«å˜ç´”ã§ã™ã€‚ãƒŸãƒ‹ãƒãƒƒãƒã®ã‚µã‚¤ã‚ºã‚’32ã¨ã—ã¾ã™ã€‚çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã«ã¯ã€é·ç§»ã‚’ä¾‹ãˆã°1,000å€‹ã»ã©å…ˆã«è²¯ã‚ã¦ãŠãã¾ã™ã€‚**DQNã®å­¦ç¿’æ™‚ã¯çµŒé¨“ãƒãƒƒãƒ•ã‚¡ã‹ã‚‰32å€‹ã®é·ç§»ã‚’ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã™ã‚‹ã“ã¨ã§ã€é·ç§»åŒå£«ã®ç›¸é–¢ã‚’æŒãŸãªã„ãƒŸãƒ‹ãƒãƒƒãƒ**ã‚’ä½œã‚Œã¾ã™ã€‚ã‚ã¨ã¯ã€å‰è¿°ã—ãŸæå¤±é–¢æ•°ã«å¾“ã£ã¦ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’å­¦ç¿’ã•ã›ã‚‹ã ã‘ã§ã™ã€‚

### 2.3 ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯

TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯$r_t+\gamma \max_{a'} Q(s_{t+1}, a';\theta_t)$ã§è¡¨ã›ã¾ã—ãŸã€‚ã“ã®TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã«ã¯ã‚ã‚‹å•é¡ŒãŒã‚ã‚Šã¾ã™ã€‚ãã‚Œã¯ã€TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®ä¸­ã«ã€æ›´æ–°å¯¾è±¡ã§ã‚ã‚‹$Q(s_{t+1}, a';\theta_t)$ãŒå«ã¾ã‚Œã¦ã„ã‚‹ã“ã¨ã§ã™ã€‚ã“ã‚Œã«ã‚ˆã‚Š$Q(\theta_t)$ãŒæ›´æ–°ã•ã‚Œã‚‹ã”ã¨ã«ã‚¿ãƒ¼ã‚²ãƒƒãƒˆã‚‚å¤§ããå¤‰åŒ–ã—ã¦ã—ã¾ã„ã€å­¦ç¿’ãŒå®‰å®šã—ã¾ã›ã‚“ã€‚ãã®ãŸã‚ã€æ›´æ–°å¯¾è±¡ã®ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯$Q(\theta_t)$ã¨ã¯åˆ¥ã«ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯$Q(\theta^-)$ã‚’ç”¨æ„ã—ã¾ã™ã€‚ã“ã®**ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®æ­£ä½“ã¯ã€ä»»æ„ã®ã‚¹ãƒ†ãƒƒãƒ—ã®é–“ã ã‘é‡ã¿ã‚’å›ºå®šã—ãŸ**$Q(\theta)$ ã«ãªã‚Šã¾ã™ã€‚ä¾‹ãˆã°ä»»æ„ã®ã‚¹ãƒ†ãƒƒãƒ—ã‚’100ã¨ã™ã‚‹ã¨ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã¯100ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«æ›´æ–°å¯¾è±¡ã®$Q(\theta)$ã®é‡ã¿ã¨åŒæœŸã•ã‚Œã‚‹ã€ã¨ã„ã†ã“ã¨ã§ã™ã€‚ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã‚’ç”¨ã„ãŸTDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯$r_t+\gamma \max_{a'} Q(s_{t+1}, a';\theta^-)$ã¨ãªã‚Šã€TDèª¤å·®ã¯æ¬¡ã®å¼ã«ãªã‚Šã¾ã™ã€‚å‰è¿°ã®èª¤å·®é–¢æ•°ã¨ã®é•ã„ã¯ã€TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®$\theta$ãŒ$\theta^-$ã«ãªã£ã¦ã„ã‚‹ç‚¹ã®ã¿ã§ã™ã€‚

$$
L(\theta_t) = \mathbb{E}_{(s_t,a_t,r_t,s_{t+1})} \big[ \big( r_t + \gamma \max_{a'} Q(s_{t+1},a';\theta^-) - Q(s_t, a_t; \theta) \big)^2 \big]
$$

### 2.4 å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°

æœ€å¾Œã«å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã§ã™ã€‚ã“ã¡ã‚‰ã¯å˜ã«å‹¾é…ã‚’$(-1, 1)$ã®ç¯„å›²ã«ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã™ã‚‹ã“ã¨ã§å­¦ç¿’ãŒå®‰å®šã—ãŸã€ã¨ã„ã†ã‚‚ã®ã§ã™ã€‚å®Ÿè£…ã§ã¯ã€Huber lossï¼ˆ`F.smooth_l1_loss`)ã‚’ç”¨ã„ã‚‹ã“ã¨ã§ã€å‹¾é…ã‚¯ãƒªãƒƒãƒ”ãƒ³ã‚°ã‚’å®Ÿç¾ã—ã¦ã„ã¾ã™ã€‚

## 3. Rainbow: 6ã¤ã®æ‰‹æ³•

* Double DQN
* å„ªå…ˆåº¦ä»˜ãçµŒé¨“å†ç”Ÿ
* Dueling Network
* Noisy Network
* Categorical Network
* Multi-step Learning

### 3.1 Double DQN

Double DQN[[Hasselt, H. 16]](https://arxiv.org/abs/1509.06461)ã§ã¯ã€TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å–ã‚Šæ–¹ã«ã•ã‚‰ã«å·¥å¤«ã‚’åŠ ãˆã¾ã™ã€‚DQNã§ã®TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯ã€ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯$Q(\theta^-)$ã‚’ç”¨ã„ã¦ä»¥ä¸‹ã®å¼ã§è¡¨ã›ã¾ã—ãŸã€‚

$$
 r_t + \gamma \max_{a'} Q(s_{t+1},a';\theta^-)
$$

ã“ã®å¼ã§ã€ç¬¬2é …ã®è¡Œå‹•$a'$ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯$Q(\theta^-)$ã«ã‚ˆã£ã¦æ±ºå®šã—ã¾ã™ã€‚**Double DQNã§ã¯ã€TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®è¡Œå‹•$a'$ã¯æ›´æ–°å¯¾è±¡ã§ã‚ã‚‹**$Q(\theta)$ã€**TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã®å€¤ã¯ã‚¿ãƒ¼ã‚²ãƒƒãƒˆãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯**$Q(\theta^-)$ã‚’ç”¨ã„ã‚‹ã‚ˆã†ã«ã—ã¦ã„ã¾ã™ã€‚ã¤ã¾ã‚Šã€Double DQNã®TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¯ä»¥ä¸‹ã®å¼ã¨ãªã‚Šã¾ã™ã€‚

$$
 r_t + \gamma  Q(s_{t+1}, \argmax_{a'} Q(s_{t+1},a';\theta);\theta^-)
$$



### 3.2 å„ªå…ˆåº¦ä»˜ãçµŒé¨“å†ç”Ÿ

çµŒé¨“å†ç”Ÿã§ã¯ã€ãƒãƒƒãƒ•ã‚¡å†…ã®å„çµŒé¨“ãŒä¸€æ§˜åˆ†å¸ƒã«ã‚ˆã£ã¦ãƒ©ãƒ³ãƒ€ãƒ ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã•ã‚Œã‚‹ã“ã¨ã§ã€ãƒŸãƒ‹ãƒãƒƒãƒãŒä½œã‚‰ã‚Œã¦ã„ã¾ã—ãŸã€‚ãŸã ã€çµŒé¨“ã«ã‚ˆã£ã¦å­¦ã³ç”²æ–ãŒã‚ã‚‹ã‚‚ã®ã¨ãã†ã§ãªã„ã‚‚ã®ãŒã‚ã‚‹ã¯ãšã§ã™ã€‚å­¦ç¿’æ™‚ã«ã¯ã€å­¦ã³ç”²æ–ãŒã‚ã‚‹çµŒé¨“ã‚’å„ªå…ˆçš„ã«ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€DQNã®DQNã®ã‚ˆã‚Šè‰¯ã„å­¦ç¿’ã‚’ä¿ƒã—ãŸã„ã§ã™ã€‚ã“ã®è€ƒãˆã‚’å®Ÿç¾ã•ã›ãŸã‚‚ã®ãŒã€**å„ªå…ˆåº¦ä»˜ãçµŒé¨“ãƒãƒƒãƒ•ã‚¡**ï¼ˆPrioritized Experience-Replay Bufferï¼‰[[Schaul, T. 15]](https://arxiv.org/abs/1511.05952)ã§ã™ã€‚

å„ªå…ˆåº¦ä»˜ãçµŒé¨“å†ç”Ÿã§ã¯ã€ã€Œ**TDèª¤å·®ãŒå¤§ãã„ã‚‚ã®ã“ãå­¦ã³ç”²æ–ãŒã‚ã‚‹**ã€ã¨ã—ã€å„çµŒé¨“ã«TDèª¤å·®ã«åŸºã¥ã„ãŸå„ªå…ˆåº¦ã‚’ã¤ã‘ã¦ãƒãƒƒãƒ•ã‚¡ã¸ä¿å­˜ã—ã¾ã™ã€‚ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã®å­¦ç¿’æ™‚ã«ã¯ã€ã“ã®å„ªå…ˆåº¦ã«åŸºã¥ã„ã¦çµŒé¨“ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã€ãƒŸãƒ‹ãƒãƒƒãƒã‚’ä½œã‚Šã¾ã™ã€‚

### 3.3 Dueling Network

Dueling Network[[Wang, Z. 15]](https://arxiv.org/abs/1511.06581)ã§ã¯ã‚ã‚‰ãŸã«**ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸**ï¼ˆAdvantageï¼‰ã€ã¨ã„ã†ã®ã‚’å°å…¥ã—ã¾ã™ã€‚ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã¯ã€è¡Œå‹•ä¾¡å€¤ã‹ã‚‰çŠ¶æ…‹ä¾¡å€¤ã‚’å¼•ã„ãŸå€¤ã«ãªã‚Šã¾ã™ã€‚ã¤ã¾ã‚Šã€ä»¥ä¸‹ã®ã‚ˆã†ã«å®šç¾©ã•ã‚Œã¾ã™ã€‚

$$
A(s,a) = Q(s, a) - V(s)
$$

è¡Œå‹•ä¾¡å€¤ã¯ã€ã‚ã‚‹çŠ¶æ…‹ã§ã‚ã‚‹è¡Œå‹•ã‚’ã¨ã£ãŸã¨ãã«å¾—ã‚‰ã‚Œã‚‹ã§ã‚ã‚ã†åç›Šã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚çŠ¶æ…‹ä¾¡å€¤ã¯ã€ãã®çŠ¶æ…‹è‡ªä½“ã§å¾—ã‚‰ã‚Œã‚‹ã§ã‚ã‚ã†åç›Šã‚’è¡¨ã—ã¦ãã‚Œã¦ã„ã¾ã™ã€‚ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã¯ã€è¡Œå‹•ä¾¡å€¤ã‹ã‚‰çŠ¶æ…‹ä¾¡å€¤ã‚’å¼•ã„ãŸå€¤ãªã®ã§ã€**ã‚ã‚‹è¡Œå‹•ãŒãã®çŠ¶æ…‹ã«ãŠã„ã¦è‰¯ã„ã‚‚ã®ãªã®ã‹æ‚ªã„ã‚‚ã®ãªã®ã‹ã‚’ç¤ºã—ã¦ãã‚Œã¦ã„ã‚‹ã“ã¨**ãŒã‚ã‹ã‚Šã¾ã™ã€‚ä¸Šã®å¼ã§$Q=$ã®å½¢ã«ç›´ã—ã¾ã™ã€‚

$$
Q(s, a) = V(s) + A(s,a) 
$$

Qå€¤ã¯ã€çŠ¶æ…‹ä¾¡å€¤ã¨ã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ã®å’Œã¨ã—ã¦ã‚ã‚‰ã‚ã›ã‚‹ã®ã§ã€ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯ã®å½¢ã‚‚ã“ã‚Œã«åˆã‚ã›ã¦å¤‰æ›´ã—ã¾ã™ã€‚

![Dueling](img/rainbow/dueling.png)

> é€šå¸¸ã®DQNï¼ˆä¸Šï¼‰ã¨Dueling Networkï¼ˆä¸‹ï¼‰ï¼ˆ[[Wang, Z. 15]](https://arxiv.org/abs/1511.06581) Figure.1ï¼‰

ä¸Šã®å›³ã«ãŠã„ã¦ã€å›³ä¸­ä¸ŠãŒé€šå¸¸ã®DQNã€å›³ä¸­ä¸‹ãŒDueling Networkã«ãªã‚Šã¾ã™ã€‚DQNã§ã¯å˜ã«Qå€¤ã‚’å‡ºã—ã¦ã„ã¾ã—ãŸã€‚ä¸€æ–¹ã€**Dueling Networkã§ã¯2è‚¡ã«åˆ†ã‘ã¦ã‹ã‚‰ãã‚Œã‚‰ã®å‡ºåŠ›ã‚’è¶³ã—ç®—ã™ã‚‹å½¢ã§Qå€¤**ã‚’å‡ºã—ã¦ã„ã¾ã™ã€‚2è‚¡ã®ã†ã¡ä¸ŠãŒçŠ¶æ…‹ä¾¡å€¤ï¼ˆã‚¹ã‚«ãƒ©ãƒ¼ï¼‰ã€ä¸‹ãŒã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸ï¼ˆãƒ™ã‚¯ãƒˆãƒ«ï¼‰ã«ãªã‚Šã€ç·‘è‰²ã®ç·šãŒã“ã‚Œã‚‰ã®å’Œã‚’è¡¨ã—ã¦ã„ã¾ã™ã€‚æœ€çµ‚çš„ãªå‡ºåŠ›ã¯Qå€¤ã«ãªã£ã¦ã„ã‚‹ã®ã§ã€æå¤±é–¢æ•°ãªã©ã¯DQNã‹ã‚‰å¤‰æ›´ãªããã®ã¾ã¾ä½¿ãˆã¾ã™ã€‚

### 3.4 Noisy Network

DQNã§ã¯ã€æ¢ç´¢ã§ã¯$\varepsilon$-greedyã‚’ç”¨ã„ã¦çµŒé¨“ã‚’è²¯ã‚ã¦ã„ã¾ã—ãŸã€‚ãŸã ã€ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã§ã‚ã‚‹$\varepsilon$ã‚’é©åˆ‡ãªå€¤ã«è¨­å®šã™ã‚‹ã®ã¯é›£ã—ã„ã§ã™ã€‚ãã“ã§ã€Noisy Network[[Fortunato, M. 18]](https://arxiv.org/abs/1706.10295)ã¯ã€**ç·šå½¢å±¤ã®é‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã«ãƒã‚¤ã‚ºã‚’åŠ ãˆã‚‹ã“ã¨ã§ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æŒãŸã›ã€$\varepsilon$-greedyã®å¿…è¦æ€§ã‚’æ’é™¤**ã—ã¦ã„ã¾ã™ã€‚

é€šå¸¸ã®ç·šå½¢å±¤ã¯ä»¥ä¸‹ã®å¼ã§ã‹ã‘ã¾ã™ã€‚

$$
y = wx + b
$$

ã“ã“ã§$x \in \mathbb{R}^p$ã¯å…¥åŠ›ã€ $w \in \mathbb{R}^{q \times p}$ã¨$b \in \mathbb{R}^{q}$ã¯ãã‚Œãã‚Œé‡ã¿ã¨ãƒã‚¤ã‚¢ã‚¹ã«ãªã‚Šã¾ã™ã€‚ä¸€æ–¹ã€Noisy Networkã®1å±¤ã¯ä»¥ä¸‹ã®å¼ã«ãªã‚Šã¾ã™ã€‚

$$
y = (\mu^w + \sigma^w \odot \epsilon^w) x + (\mu^b + \sigma^b \odot \epsilon^b)
$$

$w$ã¨$b$ãŒãã‚Œãã‚Œã€$\mu^w + \sigma^w \odot \epsilon^w$ ã¨ $\mu^b + \sigma^b \odot \epsilon^b$ ã«ç½®ãæ›ã‚ã£ã¦ã„ã‚‹ã ã‘ã§ã™ã€‚$\mu,\sigma$ãŒå­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã«ãªã£ã¦ãŠã‚Šã€$\epsilon$ã¯ãƒã‚¤ã‚ºã§ã™ã€‚ä¸Šã®å¼ã‹ã‚‰ã‚‚ã‚ã‹ã‚‹ã‚ˆã†ã«ã€Noisy Networkã§ã¯VAEã¨åŒæ§˜ã€**Reparameterization Trickã‚’ç”¨ã„ã¦é‡ã¿ãŠã‚ˆã³ãƒã‚¤ã‚¢ã‚¹ã«ãƒ©ãƒ³ãƒ€ãƒ æ€§ã‚’æŒãŸã›ã¦ã„ã¾ã™**ã€‚

### 3.5 Categorical Network

...

### 3.6 Multi-step Learning

æœ€å¾Œã«ã€Multi-step Learning[[]]()ã§ã™ã€‚ã“ã¡ã‚‰ã¯éå¸¸ã«å˜ç´”ã§ã™ã€‚DQNã§ã¯ã€æ™‚åˆ»ã‚’1ã‚¹ãƒ†ãƒƒãƒ—ã ã‘å±•é–‹ã—ãŸåç›Šã‚’TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã—ã¦ç”¨ã„ã¦ã„ã¾ã—ãŸã€‚ï¼ˆ$r_t, s_{t+1}$ã¯æ—¢çŸ¥ï¼‰

$$
r_t + \gamma \max_{a'} Q(s_{t+1}, a';\theta^{-})
$$

ã“ã‚Œã‚’ä¾‹ãˆã°2ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§å±•é–‹ã™ã‚‹ã¨ã€ä»¥ä¸‹ã®å¼ã«ãªã‚Šã¾ã™ã€‚ï¼ˆ$r_t, r_{t+1}, s_{t+2}$ã¯æ—¢çŸ¥ï¼‰

$$
r_t + \gamma r_{t+1} + \gamma^2 \max_{a'} Q(s_{t+2}, a';\theta^{-})
$$

Multi-step Learningã§ã¯ã€**$N$ã‚¹ãƒ†ãƒƒãƒ—å…ˆã¾ã§å±•é–‹ã—ãŸåç›Šã‚’TDã‚¿ãƒ¼ã‚²ãƒƒãƒˆã¨ã™ã‚‹**ã ã‘ã§ã™ã€‚Nã‚¹ãƒ†ãƒƒãƒ—å…ˆã®æ—¢çŸ¥ã®åç›Šã¯ä»¥ä¸‹ã®å¼ã§è¡¨ã›ã¾ã™ã€‚

$$
R^{n}_t = \sum_{k=0}^{n-1} \gamma^{k} r_{t+k}.
$$

ã—ãŸãŒã£ã¦ã€ä»¥ä¸‹ã®æå¤±é–¢æ•°ã§ãƒ‹ãƒ¥ãƒ¼ãƒ©ãƒ«ãƒãƒƒãƒˆã‚’å­¦ç¿’ã•ã›ã‚‹ã®ãŒã€Multi-step Learningã«ãªã‚Šã¾ã™ã€‚

$$
\mathbb{E}\left[\left(R^{n}_t + \gamma^{n} \max_{a'} Q
(s_{t+n}, a';\theta^{-}) - Q(s_t, a_t;\theta)\right)^2\right]
$$


### 4. å®Ÿè£…

## The neural network

The neural network needs to take in a noised image at a particular time step and return the predicted noise. Note that the predicted noise is a tensor that has the same size/resolution as the input image. So technically, the network takes in and outputs tensors of the same shape. What type of neural network can we use for this? 

What is typically used here is very similar to that of an [Autoencoder](https://en.wikipedia.org/wiki/Autoencoder), which you may remember from typical "intro to deep learning" tutorials. Autoencoders have a so-called "bottleneck" layer in between the encoder and decoder. The encoder first encodes an image into a smaller hidden representation called the "bottleneck", and the decoder then decodes that hidden representation back into an actual image. This forces the network to only keep the most important information in the bottleneck layer.

In terms of architecture, the DDPM authors went for a **U-Net**, introduced by ([Ronneberger et al., 2015](https://arxiv.org/abs/1505.04597)) (which, at the time, achieved state-of-the-art results for medical image segmentation). This network, like any autoencoder, consists of a bottleneck in the middle that makes sure the network learns only the most important information. Importantly, it introduced residual connections between the encoder and decoder, greatly improving gradient flow (inspired by ResNet in [He et al., 2015](https://arxiv.org/abs/1512.03385)).

<p align="center">
    <img src="assets/78_annotated-diffusion/unet_architecture.jpg" width="400" />
</p>

As can be seen, a U-Net model first downsamples the input (i.e. makes the input smaller in terms of spatial resolution), after which upsampling is performed.

Below, we implement this network, step-by-step.

### Network helpers

First, we define some helper functions and classes which will be used when implementing the neural network. Importantly, we define a `Residual` module, which simply adds the input to the output of a particular function (in other words, adds a residual connection to a particular function).

We also define aliases for the up- and downsampling operations.

```python
def exists(x):
    return x is not None

def default(val, d):
    if exists(val):
        return val
    return d() if isfunction(d) else d


def num_to_groups(num, divisor):
    groups = num // divisor
    remainder = num % divisor
    arr = [divisor] * groups
    if remainder > 0:
        arr.append(remainder)
    return arr


class Residual(nn.Module):
    def __init__(self, fn):
        super().__init__()
        self.fn = fn

    def forward(self, x, *args, **kwargs):
        return self.fn(x, *args, **kwargs) + x


def Upsample(dim, dim_out=None):
    return nn.Sequential(
        nn.Upsample(scale_factor=2, mode="nearest"),
        nn.Conv2d(dim, default(dim_out, dim), 3, padding=1),
    )


def Downsample(dim, dim_out=None):
    # No More Strided Convolutions or Pooling
    return nn.Sequential(
        Rearrange("b c (h p1) (w p2) -> b (c p1 p2) h w", p1=2, p2=2),
        nn.Conv2d(dim * 4, default(dim_out, dim), 1),
    )
```

### Position embeddings

As the parameters of the neural network are shared across time (noise level), the authors employ sinusoidal position embeddings to encode \\(t\\), inspired by the Transformer ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)). This makes the neural network "know" at which particular time step (noise level) it is operating, for every image in a batch.

The `SinusoidalPositionEmbeddings` module takes a tensor of shape `(batch_size, 1)` as input (i.e. the noise levels of several noisy images in a batch), and turns this into a tensor of shape `(batch_size, dim)`, with `dim` being the dimensionality of the position embeddings. This is then added to each residual block, as we will see further.

```python
class SinusoidalPositionEmbeddings(nn.Module):
    def __init__(self, dim):
        super().__init__()
        self.dim = dim

    def forward(self, time):
        device = time.device
        half_dim = self.dim // 2
        embeddings = math.log(10000) / (half_dim - 1)
        embeddings = torch.exp(torch.arange(half_dim, device=device) * -embeddings)
        embeddings = time[:, None] * embeddings[None, :]
        embeddings = torch.cat((embeddings.sin(), embeddings.cos()), dim=-1)
        return embeddings
```

### ResNet block

Next, we define the core building block of the U-Net model. The DDPM authors employed a Wide ResNet block ([Zagoruyko et al., 2016](https://arxiv.org/abs/1605.07146)), but Phil Wang has replaced the standard convolutional layer by a "weight standardized" version, which works better in combination with group normalization (see ([Kolesnikov et al., 2019](https://arxiv.org/abs/1912.11370)) for details).


```python
class WeightStandardizedConv2d(nn.Conv2d):
    """
    https://arxiv.org/abs/1903.10520
    weight standardization purportedly works synergistically with group normalization
    """

    def forward(self, x):
        eps = 1e-5 if x.dtype == torch.float32 else 1e-3

        weight = self.weight
        mean = reduce(weight, "o ... -> o 1 1 1", "mean")
        var = reduce(weight, "o ... -> o 1 1 1", partial(torch.var, unbiased=False))
        normalized_weight = (weight - mean) * (var + eps).rsqrt()

        return F.conv2d(
            x,
            normalized_weight,
            self.bias,
            self.stride,
            self.padding,
            self.dilation,
            self.groups,
        )


class Block(nn.Module):
    def __init__(self, dim, dim_out, groups=8):
        super().__init__()
        self.proj = WeightStandardizedConv2d(dim, dim_out, 3, padding=1)
        self.norm = nn.GroupNorm(groups, dim_out)
        self.act = nn.SiLU()

    def forward(self, x, scale_shift=None):
        x = self.proj(x)
        x = self.norm(x)

        if exists(scale_shift):
            scale, shift = scale_shift
            x = x * (scale + 1) + shift

        x = self.act(x)
        return x


class ResnetBlock(nn.Module):
    """https://arxiv.org/abs/1512.03385"""

    def __init__(self, dim, dim_out, *, time_emb_dim=None, groups=8):
        super().__init__()
        self.mlp = (
            nn.Sequential(nn.SiLU(), nn.Linear(time_emb_dim, dim_out * 2))
            if exists(time_emb_dim)
            else None
        )

        self.block1 = Block(dim, dim_out, groups=groups)
        self.block2 = Block(dim_out, dim_out, groups=groups)
        self.res_conv = nn.Conv2d(dim, dim_out, 1) if dim != dim_out else nn.Identity()

    def forward(self, x, time_emb=None):
        scale_shift = None
        if exists(self.mlp) and exists(time_emb):
            time_emb = self.mlp(time_emb)
            time_emb = rearrange(time_emb, "b c -> b c 1 1")
            scale_shift = time_emb.chunk(2, dim=1)

        h = self.block1(x, scale_shift=scale_shift)
        h = self.block2(h)
        return h + self.res_conv(x)
```

### Attention module

Next, we define the attention module, which the DDPM authors added in between the convolutional blocks. Attention is the building block of the famous Transformer architecture ([Vaswani et al., 2017](https://arxiv.org/abs/1706.03762)), which has shown great success in various domains of AI, from NLP and vision to [protein folding](https://www.deepmind.com/blog/alphafold-a-solution-to-a-50-year-old-grand-challenge-in-biology). Phil Wang employs 2 variants of attention: one is regular multi-head self-attention (as used in the Transformer), the other one is a [linear attention variant](https://github.com/lucidrains/linear-attention-transformer) ([Shen et al., 2018](https://arxiv.org/abs/1812.01243)), whose time- and memory requirements scale linear in the sequence length, as opposed to quadratic for regular attention.

For an extensive explanation of the attention mechanism, we refer the reader to Jay Allamar's [wonderful blog post](https://jalammar.github.io/illustrated-transformer/).

```python
class Attention(nn.Module):
    def __init__(self, dim, heads=4, dim_head=32):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        hidden_dim = dim_head * heads
        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)
        self.to_out = nn.Conv2d(hidden_dim, dim, 1)

    def forward(self, x):
        b, c, h, w = x.shape
        qkv = self.to_qkv(x).chunk(3, dim=1)
        q, k, v = map(
            lambda t: rearrange(t, "b (h c) x y -> b h c (x y)", h=self.heads), qkv
        )
        q = q * self.scale

        sim = einsum("b h d i, b h d j -> b h i j", q, k)
        sim = sim - sim.amax(dim=-1, keepdim=True).detach()
        attn = sim.softmax(dim=-1)

        out = einsum("b h i j, b h d j -> b h i d", attn, v)
        out = rearrange(out, "b h (x y) d -> b (h d) x y", x=h, y=w)
        return self.to_out(out)

class LinearAttention(nn.Module):
    def __init__(self, dim, heads=4, dim_head=32):
        super().__init__()
        self.scale = dim_head**-0.5
        self.heads = heads
        hidden_dim = dim_head * heads
        self.to_qkv = nn.Conv2d(dim, hidden_dim * 3, 1, bias=False)

        self.to_out = nn.Sequential(nn.Conv2d(hidden_dim, dim, 1), 
                                    nn.GroupNorm(1, dim))

    def forward(self, x):
        b, c, h, w = x.shape
        qkv = self.to_qkv(x).chunk(3, dim=1)
        q, k, v = map(
            lambda t: rearrange(t, "b (h c) x y -> b h c (x y)", h=self.heads), qkv
        )

        q = q.softmax(dim=-2)
        k = k.softmax(dim=-1)

        q = q * self.scale
        context = torch.einsum("b h d n, b h e n -> b h d e", k, v)

        out = torch.einsum("b h d e, b h d n -> b h e n", context, q)
        out = rearrange(out, "b h c (x y) -> b (h c) x y", h=self.heads, x=h, y=w)
        return self.to_out(out)
```

### Group normalization

The DDPM authors interleave the convolutional/attention layers of the U-Net with group normalization ([Wu et al., 2018](https://arxiv.org/abs/1803.08494)). Below, we define a `PreNorm` class, which will be used to apply groupnorm before the attention layer, as we'll see further. Note that there's been a [debate](https://tnq177.github.io/data/transformers_without_tears.pdf) about whether to apply normalization before or after attention in Transformers.

```python
class PreNorm(nn.Module):
    def __init__(self, dim, fn):
        super().__init__()
        self.fn = fn
        self.norm = nn.GroupNorm(1, dim)

    def forward(self, x):
        x = self.norm(x)
        return self.fn(x)
```

### Conditional U-Net

Now that we've defined all building blocks (position embeddings, ResNet blocks, attention and group normalization), it's time to define the entire neural network. Recall that the job of the network \\(\mathbf{\epsilon}_\theta(\mathbf{x}_t, t)\\) is to take in a batch of noisy images and their respective noise levels, and output the noise added to the input. More formally:

- the network takes a batch of noisy images of shape `(batch_size, num_channels, height, width)` and a batch of noise levels of shape `(batch_size, 1)` as input, and returns a tensor of shape `(batch_size, num_channels, height, width)`

The network is built up as follows:
* first, a convolutional layer is applied on the batch of noisy images, and position embeddings are computed for the noise levels
* next, a sequence of downsampling stages are applied. Each downsampling stage consists of 2 ResNet blocks + groupnorm + attention + residual connection + a downsample operation
* at the middle of the network, again ResNet blocks are applied, interleaved with attention
* next, a sequence of upsampling stages are applied. Each upsampling stage consists of 2 ResNet  blocks + groupnorm + attention + residual connection + an upsample operation
* finally, a ResNet block followed by a convolutional layer is applied.

Ultimately, neural networks stack up layers as if they were lego blocks (but it's important to [understand how they work](http://karpathy.github.io/2019/04/25/recipe/)).


```python
class Unet(nn.Module):
    def __init__(
        self,
        dim,
        init_dim=None,
        out_dim=None,
        dim_mults=(1, 2, 4, 8),
        channels=3,
        self_condition=False,
        resnet_block_groups=4,
    ):
        super().__init__()

        # determine dimensions
        self.channels = channels
        self.self_condition = self_condition
        input_channels = channels * (2 if self_condition else 1)

        init_dim = default(init_dim, dim)
        self.init_conv = nn.Conv2d(input_channels, init_dim, 1, padding=0) # changed to 1 and 0 from 7,3

        dims = [init_dim, *map(lambda m: dim * m, dim_mults)]
        in_out = list(zip(dims[:-1], dims[1:]))

        block_klass = partial(ResnetBlock, groups=resnet_block_groups)

        # time embeddings
        time_dim = dim * 4

        self.time_mlp = nn.Sequential(
            SinusoidalPositionEmbeddings(dim),
            nn.Linear(dim, time_dim),
            nn.GELU(),
            nn.Linear(time_dim, time_dim),
        )

        # layers
        self.downs = nn.ModuleList([])
        self.ups = nn.ModuleList([])
        num_resolutions = len(in_out)

        for ind, (dim_in, dim_out) in enumerate(in_out):
            is_last = ind >= (num_resolutions - 1)

            self.downs.append(
                nn.ModuleList(
                    [
                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),
                        block_klass(dim_in, dim_in, time_emb_dim=time_dim),
                        Residual(PreNorm(dim_in, LinearAttention(dim_in))),
                        Downsample(dim_in, dim_out)
                        if not is_last
                        else nn.Conv2d(dim_in, dim_out, 3, padding=1),
                    ]
                )
            )

        mid_dim = dims[-1]
        self.mid_block1 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)
        self.mid_attn = Residual(PreNorm(mid_dim, Attention(mid_dim)))
        self.mid_block2 = block_klass(mid_dim, mid_dim, time_emb_dim=time_dim)

        for ind, (dim_in, dim_out) in enumerate(reversed(in_out)):
            is_last = ind == (len(in_out) - 1)

            self.ups.append(
                nn.ModuleList(
                    [
                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),
                        block_klass(dim_out + dim_in, dim_out, time_emb_dim=time_dim),
                        Residual(PreNorm(dim_out, LinearAttention(dim_out))),
                        Upsample(dim_out, dim_in)
                        if not is_last
                        else nn.Conv2d(dim_out, dim_in, 3, padding=1),
                    ]
                )
            )

        self.out_dim = default(out_dim, channels)

        self.final_res_block = block_klass(dim * 2, dim, time_emb_dim=time_dim)
        self.final_conv = nn.Conv2d(dim, self.out_dim, 1)

    def forward(self, x, time, x_self_cond=None):
        if self.self_condition:
            x_self_cond = default(x_self_cond, lambda: torch.zeros_like(x))
            x = torch.cat((x_self_cond, x), dim=1)

        x = self.init_conv(x)
        r = x.clone()

        t = self.time_mlp(time)

        h = []

        for block1, block2, attn, downsample in self.downs:
            x = block1(x, t)
            h.append(x)

            x = block2(x, t)
            x = attn(x)
            h.append(x)

            x = downsample(x)

        x = self.mid_block1(x, t)
        x = self.mid_attn(x)
        x = self.mid_block2(x, t)

        for block1, block2, attn, upsample in self.ups:
            x = torch.cat((x, h.pop()), dim=1)
            x = block1(x, t)

            x = torch.cat((x, h.pop()), dim=1)
            x = block2(x, t)
            x = attn(x)

            x = upsample(x)

        x = torch.cat((x, r), dim=1)

        x = self.final_res_block(x, t)
        return self.final_conv(x)
```

## Defining the forward diffusion process

The forward diffusion process gradually adds noise to an image from the real distribution, in a number of time steps \\(T\\). This happens according to a **variance schedule**. The original DDPM authors employed a linear schedule:

> We set the forward process variances to constants
increasing linearly from \\(\beta_1 = 10^{âˆ’4}\\)
to \\(\beta_T = 0.02\\).

However, it was shown in ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)) that better results can be achieved when employing a cosine schedule. 

Below, we define various schedules for the \\(T\\) timesteps (we'll choose one later on).

```python
def cosine_beta_schedule(timesteps, s=0.008):
    """
    cosine schedule as proposed in https://arxiv.org/abs/2102.09672
    """
    steps = timesteps + 1
    x = torch.linspace(0, timesteps, steps)
    alphas_cumprod = torch.cos(((x / timesteps) + s) / (1 + s) * torch.pi * 0.5) ** 2
    alphas_cumprod = alphas_cumprod / alphas_cumprod[0]
    betas = 1 - (alphas_cumprod[1:] / alphas_cumprod[:-1])
    return torch.clip(betas, 0.0001, 0.9999)

def linear_beta_schedule(timesteps):
    beta_start = 0.0001
    beta_end = 0.02
    return torch.linspace(beta_start, beta_end, timesteps)

def quadratic_beta_schedule(timesteps):
    beta_start = 0.0001
    beta_end = 0.02
    return torch.linspace(beta_start**0.5, beta_end**0.5, timesteps) ** 2

def sigmoid_beta_schedule(timesteps):
    beta_start = 0.0001
    beta_end = 0.02
    betas = torch.linspace(-6, 6, timesteps)
    return torch.sigmoid(betas) * (beta_end - beta_start) + beta_start
```

To start with, let's use the linear schedule for \\(T=300\\) time steps and define the various variables from the \\(\beta_t\\) which we will need, such as the cumulative product of the variances \\(\bar{\alpha}_t\\). Each of the variables below are just 1-dimensional tensors, storing values from \\(t\\) to \\(T\\). Importantly, we also define an `extract` function, which will allow us to extract the appropriate \\(t\\) index for a batch of indices.

```python
timesteps = 300

# define beta schedule
betas = linear_beta_schedule(timesteps=timesteps)

# define alphas 
alphas = 1. - betas
alphas_cumprod = torch.cumprod(alphas, axis=0)
alphas_cumprod_prev = F.pad(alphas_cumprod[:-1], (1, 0), value=1.0)
sqrt_recip_alphas = torch.sqrt(1.0 / alphas)

# calculations for diffusion q(x_t | x_{t-1}) and others
sqrt_alphas_cumprod = torch.sqrt(alphas_cumprod)
sqrt_one_minus_alphas_cumprod = torch.sqrt(1. - alphas_cumprod)

# calculations for posterior q(x_{t-1} | x_t, x_0)
posterior_variance = betas * (1. - alphas_cumprod_prev) / (1. - alphas_cumprod)

def extract(a, t, x_shape):
    batch_size = t.shape[0]
    out = a.gather(-1, t.cpu())
    return out.reshape(batch_size, *((1,) * (len(x_shape) - 1))).to(t.device)
```

We'll illustrate with a cats image how noise is added at each time step of the diffusion process.

```python
from PIL import Image
import requests

url = 'http://images.cocodataset.org/val2017/000000039769.jpg'
image = Image.open(requests.get(url, stream=True).raw)
image
```
<img src="assets/78_annotated-diffusion/output_cats.jpeg" width="400" />

Noise is added to PyTorch tensors, rather than Pillow Images. We'll first define image transformations that allow us to go from a PIL image to a PyTorch tensor (on which we can add the noise), and vice versa.

These transformations are fairly simple: we first normalize images by dividing by \\(255\\) (such that they are in the \\([0,1]\\) range), and then make sure they are in the \\([-1, 1]\\) range. From the DPPM paper:

> We assume that image data consists of integers in \\(\{0, 1, ... , 255\}\\) scaled linearly to \\([âˆ’1, 1]\\). This
ensures that the neural network reverse process operates on consistently scaled inputs starting from
the standard normal prior \\(p(\mathbf{x}_T )\\). 


```python
from torchvision.transforms import Compose, ToTensor, Lambda, ToPILImage, CenterCrop, Resize

image_size = 128
transform = Compose([
    Resize(image_size),
    CenterCrop(image_size),
    ToTensor(), # turn into Numpy array of shape HWC, divide by 255
    Lambda(lambda t: (t * 2) - 1),
    
])

x_start = transform(image).unsqueeze(0)
x_start.shape
```

<div class="output stream stdout">

    Output:
    ----------------------------------------------------------------------------------------------------
    torch.Size([1, 3, 128, 128])

</div>

We also define the reverse transform, which takes in a PyTorch tensor containing values in \\([-1, 1]\\) and turn them back into a PIL image:

```python
import numpy as np

reverse_transform = Compose([
     Lambda(lambda t: (t + 1) / 2),
     Lambda(lambda t: t.permute(1, 2, 0)), # CHW to HWC
     Lambda(lambda t: t * 255.),
     Lambda(lambda t: t.numpy().astype(np.uint8)),
     ToPILImage(),
])
```

Let's verify this:

```python
reverse_transform(x_start.squeeze())
```
    
<img src="assets/78_annotated-diffusion/output_cats_verify.png" width="100" />

We can now define the forward diffusion process as in the paper:


```python
# forward diffusion (using the nice property)
def q_sample(x_start, t, noise=None):
    if noise is None:
        noise = torch.randn_like(x_start)

    sqrt_alphas_cumprod_t = extract(sqrt_alphas_cumprod, t, x_start.shape)
    sqrt_one_minus_alphas_cumprod_t = extract(
        sqrt_one_minus_alphas_cumprod, t, x_start.shape
    )

    return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise
```

Let's test it on a particular time step:

```python
def get_noisy_image(x_start, t):
  # add noise
  x_noisy = q_sample(x_start, t=t)

  # turn back into PIL image
  noisy_image = reverse_transform(x_noisy.squeeze())

  return noisy_image
```

```python
# take time step
t = torch.tensor([40])

get_noisy_image(x_start, t)
```

<img src="assets/78_annotated-diffusion/output_cats_noisy.png" width="100" />

Let's visualize this for various time steps:

```python
import matplotlib.pyplot as plt

# use seed for reproducability
torch.manual_seed(0)

# source: https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py
def plot(imgs, with_orig=False, row_title=None, **imshow_kwargs):
    if not isinstance(imgs[0], list):
        # Make a 2d grid even if there's just 1 row
        imgs = [imgs]

    num_rows = len(imgs)
    num_cols = len(imgs[0]) + with_orig
    fig, axs = plt.subplots(figsize=(200,200), nrows=num_rows, ncols=num_cols, squeeze=False)
    for row_idx, row in enumerate(imgs):
        row = [image] + row if with_orig else row
        for col_idx, img in enumerate(row):
            ax = axs[row_idx, col_idx]
            ax.imshow(np.asarray(img), **imshow_kwargs)
            ax.set(xticklabels=[], yticklabels=[], xticks=[], yticks=[])

    if with_orig:
        axs[0, 0].set(title='Original image')
        axs[0, 0].title.set_size(8)
    if row_title is not None:
        for row_idx in range(num_rows):
            axs[row_idx, 0].set(ylabel=row_title[row_idx])

    plt.tight_layout()
```

```python
plot([get_noisy_image(x_start, torch.tensor([t])) for t in [0, 50, 100, 150, 199]])
```
  
<img src="assets/78_annotated-diffusion/output_cats_noisy_multiple.png" width="800" />
    
This means that we can now define the loss function given the model as follows:

```python
def p_losses(denoise_model, x_start, t, noise=None, loss_type="l1"):
    if noise is None:
        noise = torch.randn_like(x_start)

    x_noisy = q_sample(x_start=x_start, t=t, noise=noise)
    predicted_noise = denoise_model(x_noisy, t)

    if loss_type == 'l1':
        loss = F.l1_loss(noise, predicted_noise)
    elif loss_type == 'l2':
        loss = F.mse_loss(noise, predicted_noise)
    elif loss_type == "huber":
        loss = F.smooth_l1_loss(noise, predicted_noise)
    else:
        raise NotImplementedError()

    return loss
```

The `denoise_model` will be our U-Net defined above. We'll employ the Huber loss between the true and the predicted noise.

## Define a PyTorch Dataset + DataLoader

Here we define a regular [PyTorch Dataset](https://pytorch.org/tutorials/beginner/basics/data_tutorial.html). The dataset simply consists of images from a real dataset, like Fashion-MNIST, CIFAR-10 or ImageNet, scaled linearly to \\([âˆ’1, 1]\\).

Each image is resized to the same size. Interesting to note is that images are also randomly horizontally flipped. From the paper:

> We used random horizontal flips during training for CIFAR10; we tried training both with and without flips, and found flips to improve sample quality slightly.

Here we use the ğŸ¤— [Datasets library](https://huggingface.co/docs/datasets/index) to easily load the Fashion MNIST dataset from the [hub](https://huggingface.co/datasets/fashion_mnist). This dataset consists of images which already have the same resolution, namely 28x28.

```python
from datasets import load_dataset

# load dataset from the hub
dataset = load_dataset("fashion_mnist")
image_size = 28
channels = 1
batch_size = 128
```

Next, we define a function which we'll apply on-the-fly on the entire dataset. We use the `with_transform` [functionality](https://huggingface.co/docs/datasets/v2.2.1/en/package_reference/main_classes#datasets.Dataset.with_transform) for that. The function just applies some basic image preprocessing: random horizontal flips, rescaling and finally make them have values in the \\([-1,1]\\) range.

```python
from torchvision import transforms
from torch.utils.data import DataLoader

# define image transformations (e.g. using torchvision)
transform = Compose([
            transforms.RandomHorizontalFlip(),
            transforms.ToTensor(),
            transforms.Lambda(lambda t: (t * 2) - 1)
])

# define function
def transforms(examples):
   examples["pixel_values"] = [transform(image.convert("L")) for image in examples["image"]]
   del examples["image"]

   return examples

transformed_dataset = dataset.with_transform(transforms).remove_columns("label")

# create dataloader
dataloader = DataLoader(transformed_dataset["train"], batch_size=batch_size, shuffle=True)
```

```python
batch = next(iter(dataloader))
print(batch.keys())
```

<div class="output stream stdout">

    Output:
    ----------------------------------------------------------------------------------------------------
    dict_keys(['pixel_values'])

</div>


## Sampling

As we'll sample from the model during training (in order to track progress), we define the code for that below. Sampling is summarized in the paper as Algorithm 2:

<img src="assets/78_annotated-diffusion/sampling.png" width="500" />

Generating new images from a diffusion model happens by reversing the diffusion process: we start from \\(T\\), where we sample pure noise from a Gaussian distribution, and then use our neural network to gradually denoise it (using the conditional probability it has learned), until we end up at time step \\(t = 0\\). As shown above, we can derive a slighly less denoised image \\(\mathbf{x}_{t-1 }\\) by plugging in the reparametrization of the mean, using our noise predictor. Remember that the variance is known ahead of time.

Ideally, we end up with an image that looks like it came from the real data distribution.

The code below implements this.

```python
@torch.no_grad()
def p_sample(model, x, t, t_index):
    betas_t = extract(betas, t, x.shape)
    sqrt_one_minus_alphas_cumprod_t = extract(
        sqrt_one_minus_alphas_cumprod, t, x.shape
    )
    sqrt_recip_alphas_t = extract(sqrt_recip_alphas, t, x.shape)
    
    # Equation 11 in the paper
    # Use our model (noise predictor) to predict the mean
    model_mean = sqrt_recip_alphas_t * (
        x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t
    )

    if t_index == 0:
        return model_mean
    else:
        posterior_variance_t = extract(posterior_variance, t, x.shape)
        noise = torch.randn_like(x)
        # Algorithm 2 line 4:
        return model_mean + torch.sqrt(posterior_variance_t) * noise 

# Algorithm 2 (including returning all images)
@torch.no_grad()
def p_sample_loop(model, shape):
    device = next(model.parameters()).device

    b = shape[0]
    # start from pure noise (for each example in the batch)
    img = torch.randn(shape, device=device)
    imgs = []

    for i in tqdm(reversed(range(0, timesteps)), desc='sampling loop time step', total=timesteps):
        img = p_sample(model, img, torch.full((b,), i, device=device, dtype=torch.long), i)
        imgs.append(img.cpu().numpy())
    return imgs

@torch.no_grad()
def sample(model, image_size, batch_size=16, channels=3):
    return p_sample_loop(model, shape=(batch_size, channels, image_size, image_size))
```

Note that the code above is a simplified version of the original implementation. We found our simplification (which is in line with Algorithm 2 in the paper) to work just as well as the [original, more complex implementation](https://github.com/hojonathanho/diffusion/blob/master/diffusion_tf/diffusion_utils.py), which employs [clipping](https://github.com/hojonathanho/diffusion/issues/5).

## Train the model

Next, we train the model in regular PyTorch fashion. We also define some logic to periodically save generated images, using the `sample` method defined above.


```python
from pathlib import Path

def num_to_groups(num, divisor):
    groups = num // divisor
    remainder = num % divisor
    arr = [divisor] * groups
    if remainder > 0:
        arr.append(remainder)
    return arr

results_folder = Path("./results")
results_folder.mkdir(exist_ok = True)
save_and_sample_every = 1000
```

Below, we define the model, and move it to the GPU. We also define a standard optimizer (Adam).

```python
from torch.optim import Adam

device = "cuda" if torch.cuda.is_available() else "cpu"

model = Unet(
    dim=image_size,
    channels=channels,
    dim_mults=(1, 2, 4,)
)
model.to(device)

optimizer = Adam(model.parameters(), lr=1e-3)
```

Let's start training!

```python
from torchvision.utils import save_image

epochs = 6

for epoch in range(epochs):
    for step, batch in enumerate(dataloader):
      optimizer.zero_grad()

      batch_size = batch["pixel_values"].shape[0]
      batch = batch["pixel_values"].to(device)

      # Algorithm 1 line 3: sample t uniformally for every example in the batch
      t = torch.randint(0, timesteps, (batch_size,), device=device).long()

      loss = p_losses(model, batch, t, loss_type="huber")

      if step % 100 == 0:
        print("Loss:", loss.item())

      loss.backward()
      optimizer.step()

      # save generated images
      if step != 0 and step % save_and_sample_every == 0:
        milestone = step // save_and_sample_every
        batches = num_to_groups(4, batch_size)
        all_images_list = list(map(lambda n: sample(model, batch_size=n, channels=channels), batches))
        all_images = torch.cat(all_images_list, dim=0)
        all_images = (all_images + 1) * 0.5
        save_image(all_images, str(results_folder / f'sample-{milestone}.png'), nrow = 6)
```

<div class="output stream stdout">

    Output:
    ----------------------------------------------------------------------------------------------------
    Loss: 0.46477368474006653
    Loss: 0.12143351882696152
    Loss: 0.08106148988008499
    Loss: 0.0801810547709465
    Loss: 0.06122320517897606
    Loss: 0.06310459971427917
    Loss: 0.05681884288787842
    Loss: 0.05729678273200989
    Loss: 0.05497899278998375
    Loss: 0.04439849033951759
    Loss: 0.05415581166744232
    Loss: 0.06020551547408104
    Loss: 0.046830907464027405
    Loss: 0.051029372960329056
    Loss: 0.0478244312107563
    Loss: 0.046767622232437134
    Loss: 0.04305662214756012
    Loss: 0.05216279625892639
    Loss: 0.04748568311333656
    Loss: 0.05107741802930832
    Loss: 0.04588869959115982
    Loss: 0.043014321476221085
    Loss: 0.046371955424547195
    Loss: 0.04952816292643547
    Loss: 0.04472338408231735

</div>


## Sampling (inference)

To sample from the model, we can just use our sample function defined above:


```python
# sample 64 images
samples = sample(model, image_size=image_size, batch_size=64, channels=channels)

# show a random one
random_index = 5
plt.imshow(samples[-1][random_index].reshape(image_size, image_size, channels), cmap="gray")
```

<img src="assets/78_annotated-diffusion/output.png" width="300" />

Seems like the model is capable of generating a nice T-shirt! Keep in mind that the dataset we trained on is pretty low-resolution (28x28).

We can also create a gif of the denoising process:

```python
import matplotlib.animation as animation

random_index = 53

fig = plt.figure()
ims = []
for i in range(timesteps):
    im = plt.imshow(samples[i][random_index].reshape(image_size, image_size, channels), cmap="gray", animated=True)
    ims.append([im])

animate = animation.ArtistAnimation(fig, ims, interval=50, blit=True, repeat_delay=1000)
animate.save('diffusion.gif')
plt.show()
```

<img src="
assets/78_annotated-diffusion/diffusion-sweater.gif" width="300" />

## èª¬æ˜æ‰‹é †
- 07. N-Step Learning
- 03. Prioritized Replay Buffer
- 05. Noisy Network
- 04. Dueling Network 
- 01. DQN
- 02. Double Q-Learning
- 06. Categorical DQN

## Install and Import Python Library

### Install 

```
import sys
IN_COLAB = "google.colab" in sys.modules

if IN_COLAB:
    !apt install python-opengl
    !apt install ffmpeg
    !apt install xvfb
    !pip install PyVirtualDisplay==3.0
    # ä»Šå›å¼·åŒ–å­¦ç¿’ã§æ‰±ã†ã‚²ãƒ¼ãƒ ã¨ã—ã¦Curt-Poleã‚’é¸æŠã—ã¦ã„ã„ã¾ã™ã€‚OSSã®OpenAIã€ŒGymã€ã‚’ç”¨ã„ã¾ã™ã€‚
    !pip install gym==0.21.0
    from pyvirtualdisplay import Display
  
    # Start virtual display
    dis = Display(visible=0, size=(400, 400))
    dis.start()
    ! pip install pyglet==1.5.27
```

### Import 

```
import math
import os
import random
from collections import deque
# Dequeã¯
from typing import Deque, Dict, List, Tuple

import gym
import matplotlib.pyplot as plt
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from IPython.display import clear_output
from torch.nn.utils import clip_grad_norm_

# download segment tree module
# SumSegmentTreeã¯03. Prioritized Replay Bufferã§åˆ©ç”¨ã€‚
# Segment Treeã¨è¨€ã†ãƒ‡ãƒ¼ã‚¿æ§‹é€ ã‚’ã–ã£ãã‚ŠçŸ¥ã‚ŠãŸã„æ–¹ã¯ã“ã¡ã‚‰ã€‚
# - https://qiita.com/ageprocpp/items/f22040a57ad25d04d199
# - https://leetcode.com/articles/a-recursive-approach-to-segment-trees-range-sum-queries-lazy-propagation/
if IN_COLAB:
    !wget https://raw.githubusercontent.com/curt-park/rainbow-is-all-you-need/master/segment_tree.py
from segment_tree imporkt MinSegmentTree, SumSegmentTree
```

## Replay Buffer ã¨ 07. N-Step Learning

```
class ReplayBuffer:
    """A simple numpy replay buffer."""

    def __init__(
        self, 
        obs_dim: int, 
        size: int, 
        batch_size: int = 32, 
        n_step: int = 1, 
        gamma: float = 0.99
    ):
        self.obs_buf = np.zeros([size, obs_dim], dtype=np.float32)
        self.next_obs_buf = np.zeros([size, obs_dim], dtype=np.float32)
        self.acts_buf = np.zeros([size], dtype=np.float32)
        self.rews_buf = np.zeros([size], dtype=np.float32)
        self.done_buf = np.zeros(size, dtype=np.float32)
        self.max_size, self.batch_size = size, batch_size
        self.ptr, self.size, = 0, 0
        
        # for N-step Learning
        # n_step_bufferã¨è¨€ã†Nå€‹åˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’è²¯ã‚ã‚‹ã“ã¨ãŒã§ãã‚‹è²¯è”µåº«ã‚’ç”¨æ„ã™ã‚‹ã€‚
        self.n_step_buffer = deque(maxlen=n_step)
        self.n_step = n_step
        self.gamma = gamma

    ## Replay Buffer of DQN
    # def store(
    #     self,
    #     obs: np.ndarray,
    #     act: np.ndarray, 
    #     rew: float, 
    #     next_obs: np.ndarray, 
    #     done: bool,
    # ):
    #     self.obs_buf[self.ptr] = obs
    #     self.next_obs_buf[self.ptr] = next_obs
    #     self.acts_buf[self.ptr] = act
    #     self.rews_buf[self.ptr] = rew
    #     self.done_buf[self.ptr] = done
    #     # max_size(1000)ã«å¯¾ã—ã¦ã€1001å€‹ç›®ã®ãƒ‡ãƒ¼ã‚¿ãŒå…¥ã£ã¦ããŸéš›ã«ã¯1å€‹ç›®ã®ãƒ‡ãƒ¼ã‚¿ã«ä¸Šæ›¸ãã‚’è¡Œã„ã€å¸¸ã«1000ã“ã®ãƒ‡ãƒ¼ã‚¿ã‚’ä¿æœ‰ã™ã‚‹ã€‚
    #     self.ptr = (self.ptr + 1) % self.max_size
    #     self.size = min(self.size + 1, self.max_size)

    ## Replay Buffer of N-Step Learning.
    def store(
        self, 
        obs: np.ndarray, 
        act: np.ndarray, 
        rew: float, 
        next_obs: np.ndarray, 
        done: bool,
    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:
        transition = (obs, act, rew, next_obs, done)
        # è²¯è”µåº«ã®æœ«å°¾ã«1-frameåˆ†ã®ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ .
        self.n_step_buffer.append(transition)

        # single step transition is not ready
        # Nå€‹ãŒBufferã«è²¯ã¾ã‚‹ã¾ã§ã€ç©ºã®Tupleã‚’è¿”ã™ã€‚
        if len(self.n_step_buffer) < self.n_step:
            return ()
        
        # make a n-step transition
        rew, next_obs, done = self._get_n_step_info(
            self.n_step_buffer, self.gamma
        )
        obs, act = self.n_step_buffer[0][:2]
        
        self.obs_buf[self.ptr] = obs
        self.next_obs_buf[self.ptr] = next_obs
        self.acts_buf[self.ptr] = act
        self.rews_buf[self.ptr] = rew
        self.done_buf[self.ptr] = done
        self.ptr = (self.ptr + 1) % self.max_size
        self.size = min(self.size + 1, self.max_size)
        
        return self.n_step_buffer[0]

    def sample_batch(self) -> Dict[str, np.ndarray]:
        idxs = np.random.choice(self.size, size=self.batch_size, replace=False)

        return dict(
            obs=self.obs_buf[idxs], # obs is abbreviation of observations.
            next_obs=self.next_obs_buf[idxs],
            acts=self.acts_buf[idxs], # acts is abbreviations of actions.
            rews=self.rews_buf[idxs], # rews is abbreviations of rewards.
            done=self.done_buf[idxs],
            # for N-step Learning
            indices=idxs,
        )
    
    def sample_batch_from_idxs(
        self, idxs: np.ndarray
    ) -> Dict[str, np.ndarray]:
        # for N-step Learning
        return dict(
            obs=self.obs_buf[idxs],
            next_obs=self.next_obs_buf[idxs],
            acts=self.acts_buf[idxs],
            rews=self.rews_buf[idxs],
            done=self.done_buf[idxs],
        )
    
    def _get_n_step_info(
        self, n_step_buffer: Deque, gamma: float
    ) -> Tuple[np.int64, np.ndarray, bool]:
        """Return n step rew, next_obs, and done."""
        # info of the last transition
        rew, next_obs, done = n_step_buffer[-1][-3:]

        for transition in reversed(list(n_step_buffer)[:-1]):
            r, n_o, d = transition[-3:]

            rew = r + gamma * rew * (1 - d)
            next_obs, done = (n_o, d) if d else (next_obs, done)

        return rew, next_obs, done

    def __len__(self) -> int:
        return self.size
```

## 03. Prioritized Replay Buffer 

- èª¤å·®(TD-Error)ãŒå¤§ãã„ãƒ‡ãƒ¼ã‚¿ãŒç¹°ã‚Šè¿”ã—å­¦ç¿’ã•ã‚Œã‚‹ç¾è±¡ã‚’é˜²ããŸã‚ã€pure greedy prioritizationã¨uniform random samplingã‚’çµ„ã¿åˆã‚ã›ãŸStochastic samplingã¨è¨€ã†æ‰‹æ³•ã‚’æ¡ç”¨.
 
$$
P(i) = \frac{p_i^{\alpha}}{\sum_k p_k^{\alpha}}
$$

- DQNã§å°å…¥ã•ã‚ŒãŸRandom Samplingã¯ãƒ‡ãƒ¼ã‚¿é–“ã®ç›¸é–¢ã‚’æ¸›ã‚‰ã™ã“ã¨ãŒã§ãã¦ã„ãŸã€‚ã—ã‹ã—ã€Prioritized Replay Bufferã«ã‚ˆã‚ŠRandom Samplingã§ãªããªã£ãŸãŸã‚ã€å†åº¦ãƒ‡ãƒ¼ã‚¿é–“ã®ç›¸é–¢ãŒç™ºç”Ÿã€‚ã“ã®å•é¡Œã¯ã€ä»¥ä¸‹ã®importance-sampling (IS) weightsã¨è¨€ã†é‡ã¿ã‚’ç”¨ã„ãªãŒã‚‰Samplingã‚’è¡Œã†ã“ã¨ã§è§£æ¶ˆã‚’è¡Œã†ã€‚

$$
w_i = \big( \frac{1}{N} \cdot \frac{1}{P(i)} \big)^\beta
$$

```
class PrioritizedReplayBuffer(ReplayBuffer):
    """Prioritized Replay buffer.
    
    Attributes:
        max_priority (float): max priority
        tree_ptr (int): next index of tree
        alpha (float): alpha parameter for prioritized replay buffer
        sum_tree (SumSegmentTree): sum tree for prior
        min_tree (MinSegmentTree): min tree for min prior to get max weight
        
    """
    
    def __init__(
        self, 
        obs_dim: int, 
        size: int, 
        batch_size: int = 32, 
        alpha: float = 0.6,
        n_step: int = 1, 
        gamma: float = 0.99,
    ):
        """Initialization."""
        assert alpha >= 0
        
        super(PrioritizedReplayBuffer, self).__init__(
            obs_dim, size, batch_size, n_step, gamma
        )
        self.max_priority, self.tree_ptr = 1.0, 0
        self.alpha = alpha
        
        # capacity must be positive and a power of 2.
        tree_capacity = 1
        while tree_capacity < self.max_size:
            tree_capacity *= 2

        self.sum_tree = SumSegmentTree(tree_capacity)
        self.min_tree = MinSegmentTree(tree_capacity)
        
    def store(
        self, 
        obs: np.ndarray, 
        act: int, 
        rew: float, 
        next_obs: np.ndarray, 
        done: bool,
    ) -> Tuple[np.ndarray, np.ndarray, float, np.ndarray, bool]:
        """Store experience and priority."""
        transition = super().store(obs, act, rew, next_obs, done)
        
        if transition:
            self.sum_tree[self.tree_ptr] = self.max_priority ** self.alpha
            self.min_tree[self.tree_ptr] = self.max_priority ** self.alpha
            self.tree_ptr = (self.tree_ptr + 1) % self.max_size
        
        return transition

    def sample_batch(self, beta: float = 0.4) -> Dict[str, np.ndarray]:
        """Sample a batch of experiences."""
        assert len(self) >= self.batch_size
        assert beta > 0
        
        indices = self._sample_proportional()
        
        obs = self.obs_buf[indices]
        next_obs = self.next_obs_buf[indices]
        acts = self.acts_buf[indices]
        rews = self.rews_buf[indices]
        done = self.done_buf[indices]
        weights = np.array([self._calculate_weight(i, beta) for i in indices])
        
        return dict(
            obs=obs,
            next_obs=next_obs,
            acts=acts,
            rews=rews,
            done=done,
            weights=weights,
            indices=indices,
        )
        
    def update_priorities(self, indices: List[int], priorities: np.ndarray):
        # 1ãƒ•ãƒ¬ãƒ¼ãƒ ã”ã¨ã«è¡Œã‚ã‚Œã‚‹update_modelã¨è¨€ã†é–¢æ•°ã®ä¸­ã§ã€Priorityã®æ›´æ–°ã‚’è¡Œã†ã€‚
        """Update priorities of sampled transitions."""
        assert len(indices) == len(priorities)

        for idx, priority in zip(indices, priorities):
            assert priority > 0
            assert 0 <= idx < len(self)

            self.sum_tree[idx] = priority ** self.alpha
            self.min_tree[idx] = priority ** self.alpha

            self.max_priority = max(self.max_priority, priority)
            
    def _sample_proportional(self) -> List[int]:
        """Sample indices based on proportions."""
        indices = []
        p_total = self.sum_tree.sum(0, len(self) - 1)
        segment = p_total / self.batch_size
        
        for i in range(self.batch_size):
            a = segment * i
            b = segment * (i + 1)
            upperbound = random.uniform(a, b)
            idx = self.sum_tree.retrieve(upperbound)
            indices.append(idx)
            
        return indices
    
    def _calculate_weight(self, idx: int, beta: float):
        """Calculate the weight of the experience at idx."""
        # get max weight
        p_min = self.min_tree.min() / self.sum_tree.sum()
        max_weight = (p_min * len(self)) ** (-beta)
        
        # calculate weights
        p_sample = self.sum_tree[idx] / self.sum_tree.sum()
        # importance-sampling (IS) weightsã®è¨ˆç®—ã‚’è¡Œã†.
        weight = (p_sample * len(self)) ** (-beta)
        weight = weight / max_weight
        
        return weight
```

## Reference 
- [Rainbow is all you need](https://github.com/Curt-Park/rainbow-is-all-you-need)

## 05. Noisy Network


å¾“æ¥ã®DQNã§ã¯ã€ä¸€èˆ¬çš„ãªNeural Networkã‚’æ¡ç”¨ã€‚

$$
y = wx + b,
$$
x
$x \in \mathbb{R}^p$ ã¯å…¥åŠ›ã§pæ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã€$w \in \mathbb{R}^{q \times p}$ã¯é‡ã¿ã§q x pæ¬¡å…ƒã®è¡Œåˆ—ã€$b \in \mathbb{R}$ã¯ãƒã‚¤ã‚¢ã‚¹ã§qæ¬¡å…ƒã®ãƒ™ã‚¯ãƒˆãƒ«ã€‚

ã—ã‹ã—ã€NoisyNetworkã§ã¯ã€æ­£è¦åˆ†å¸ƒã®ãƒã‚¤ã‚ºã¨å­¦ç¿’å¯èƒ½ãªãƒ‘ãƒ©ãƒ¡ã‚¿ muã¨sigmaã‚’ç”¨ã„ã¦ã€$w ã¨ $b ã‚’è¡¨ç¾ã™ã‚‹ã€‚

$$
y = (\mu^w + \sigma^w \odot \epsilon^w) x + \mu^b + \sigma^b \odot \epsilon^b,
$$

```
F.linear(
            x,
            self.weight_mu + self.weight_sigma * self.weight_epsilon,
            self.bias_mu + self.bias_sigma * self.bias_epsilon,
        )
```

$\mu^w + \sigma^w \odot \epsilon^w$ ãŒ $w ã«å¯¾å¿œã—ã€
$\mu^b + \sigma^b \odot \epsilon^b$ replace $w$ ãŒ $b ã«å¯¾å¿œã™ã‚‹ã€‚

$\mu^w \in \mathbb{R}^{q \times p}, \mu^b \in \mathbb{R}^q, \sigma^w \in \mathbb{R}^{q \times p}$ and $\sigma^b \in \mathbb{R}^q$ ã¯å­¦ç¿’å¯èƒ½ã¨ãªã£ã¦ãŠã‚Šã€ $\epsilon^w \in \mathbb{R}^{q \times p}$ ã¨ $\epsilon^b \in \mathbb{R}^q$ ã¯æ­£è¦åˆ†å¸ƒã§ãƒã‚¤ã‚ºã¨ãªã‚‹ã€‚

å­¦ç¿’å¯èƒ½ã¨ã™ã‚‹ãŸã‚ã«ã€nn.Parameterã‚’ä½¿ç”¨ã™ã‚‹ã€‚
```
self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))
self.weight_sigma = nn.Parameter(
    torch.Tensor(out_features, in_features)
)
```

1. **Independent Gaussian noise**: the noise applied to each weight and bias is independent, where each random noise entry is drawn from a unit Gaussian distribution. This means that for each noisy linear layer, there are $pq + q$ noise variables (for $p$ inputs to the layer and $q$ outputs).
2. **Factorised Gaussian noise:** This is a more computationally efficient way. It produces 2 random Gaussian noise vectors ($p, q$) and makes $pq + q$ noise entries by outer product as follows:

$$
\begin{align}
\epsilon_{i,j}^w &= f(\epsilon_i) f(\epsilon_j),\\
\epsilon_{j}^b &= f(\epsilon_i),\\
\text{where } f(x) &= sgn(x) \sqrt{|x|}.
\end{align}
$$

ã¨è¨€ã†2ã¤ã®æ‰‹æ³•ãŒã‚ã‚‹ãŒã€å®Ÿé¨“çš„ã«**Factorised Gaussian noise:**ã¨è¨€ã†æ‰‹æ³•ãŒè‰¯ã„ã¨åˆ†ã‹ã£ãŸãŸã‚ã€æ¡ç”¨.

```
class NoisyLinear(nn.Module):
    """Noisy linear module for NoisyNet.
    
    
        
    Attributes:
        in_features (int): input size of linear module
        out_features (int): output size of linear module
        std_init (float): initial std value
        weight_mu (nn.Parameter): mean value weight parameter
        weight_sigma (nn.Parameter): std value weight parameter
        bias_mu (nn.Parameter): mean value bias parameter
        bias_sigma (nn.Parameter): std value bias parameter
        
    """

    def __init__(
        self, 
        in_features: int, 
        out_features: int, 
        std_init: float = 0.5,
    ):
        """Initialization."""
        super(NoisyLinear, self).__init__()
        
        self.in_features = in_features
        self.out_features = out_features
        self.std_init = std_init

        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))
        self.weight_sigma = nn.Parameter(
            torch.Tensor(out_features, in_features)
        )
        self.register_buffer(
            "weight_epsilon", torch.Tensor(out_features, in_features)
        )

        self.bias_mu = nn.Parameter(torch.Tensor(out_features))
        self.bias_sigma = nn.Parameter(torch.Tensor(out_features))
        self.register_buffer("bias_epsilon", torch.Tensor(out_features))

        self.reset_parameters()
        self.reset_noise()

    def reset_parameters(self):
        """Reset trainable network parameters (factorized gaussian noise)."""
        mu_range = 1 / math.sqrt(self.in_features)
        self.weight_mu.data.uniform_(-mu_range, mu_range)
        self.weight_sigma.data.fill_(
            self.std_init / math.sqrt(self.in_features)
        )
        self.bias_mu.data.uniform_(-mu_range, mu_range)
        self.bias_sigma.data.fill_(
            self.std_init / math.sqrt(self.out_features)
        )

    def reset_noise(self):
        """Make new noise."""
        epsilon_in = self.scale_noise(self.in_features)
        epsilon_out = self.scale_noise(self.out_features)

        # outer product
        self.weight_epsilon.copy_(epsilon_out.ger(epsilon_in))
        self.bias_epsilon.copy_(epsilon_out)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward method implementation.
        
        We don't use separate statements on train / eval mode.
        It doesn't show remarkable difference of performance.
        """
        # Noisy Networkã®è¨ˆç®—ã‚’è¡Œã†.
        return F.linear(
            x,
            self.weight_mu + self.weight_sigma * self.weight_epsilon,
            self.bias_mu + self.bias_sigma * self.bias_epsilon,
        )
    
    @staticmethod
    def scale_noise(size: int) -> torch.Tensor:
        """Set scale to make noise (factorized gaussian noise)."""
        x = torch.randn(size)

        return x.sign().mul(x.abs().sqrt())
```

## 04. Dueling Network

- ä¸‹è¨˜ã®ã‚ˆã†ãªã‚¢ãƒ‰ãƒãƒ³ãƒ†ãƒ¼ã‚¸é–¢æ•°ã‚’ã€è¡Œå‹•ä¾¡å€¤é–¢æ•° - çŠ¶æ…‹ä¾¡å€¤é–¢æ•°ã¨ã„ã†å®šç¾©ã§ä½œæˆã™ã‚‹ã€‚

![fig1](https://user-images.githubusercontent.com/14961526/60322956-c2f0b600-99bb-11e9-9ed4-443bd14bc3b0.png)

$$Q(s, a; \theta, \alpha, \beta) = V (s; \theta, \beta) + A(s, a; \theta, \alpha),$$

where $\theta$ denotes the parameters of the convolutional layers, while $\alpha$ and $\beta$ are the parameters of the two streams of fully-connected layers.

ã—ã‹ã—ã€ã“ã®çŠ¶æ³ã ã¨ã‚ã‚‹ $Q$ ã«å¯¾ã—ã¦ç„¡æ•°ã® $V$ ã¨ $A$ ã®å€™è£œãŒå­˜åœ¨ã™ã‚‹ãŸã‚ã€ä»¥ä¸‹ã®ã‚ˆã†ãªåˆ¶é™ã‚’è¨­ã‘ã‚‹ã€‚

$$
Q(s, a; \theta, \alpha, \beta) = V (s; \theta, \beta) + \big( A(s, a; \theta, \alpha) - \max_{a' \in |\mathcal{A}|} A(s, a'; \theta, \alpha) \big).
$$

ã‚ˆã‚Šå®‰å®šçš„ã«å­¦ç¿’ã‚’è¡Œã†ãŸã‚ã«ã€Maxã§ã¯ãªãå¹³å‡ã‚’ç”¨ã„ãŸæ‰‹æ³•ãŒææ¡ˆã•ã‚ŒãŸã€‚

$$
Q(s, a; \theta, \alpha, \beta) = V (s; \theta, \beta) + \big( A(s, a; \theta, \alpha) - \frac{1}{|\mathcal{A}|} \sum_{a'} A(s, a'; \theta, \alpha) \big).
$$

```
class Network(nn.Module):
    def __init__(
        self, 
        in_dim: int, 
        out_dim: int, 
        atom_size: int, 
        support: torch.Tensor
    ):
        """Initialization."""
        super(Network, self).__init__()
        
        self.support = support
        self.out_dim = out_dim
        self.atom_size = atom_size

        # set common feature layer
        self.feature_layer = nn.Sequential(
            nn.Linear(in_dim, 128), 
            nn.ReLU(),
        )
        
        # set advantage layer
        self.advantage_hidden_layer = NoisyLinear(128, 128)
        self.advantage_layer = NoisyLinear(128, out_dim * atom_size)

        # set value layer
        self.value_hidden_layer = NoisyLinear(128, 128)
        self.value_layer = NoisyLinear(128, atom_size)

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """Forward method implementation."""
        dist = self.dist(x)
        q = torch.sum(dist * self.support, dim=2)
        
        return q
    
    def dist(self, x: torch.Tensor) -> torch.Tensor:
        """Get distribution for atoms."""
        feature = self.feature_layer(x)
        adv_hid = F.relu(self.advantage_hidden_layer(feature))
        val_hid = F.relu(self.value_hidden_layer(feature))
        
        advantage = self.advantage_layer(adv_hid).view(
            -1, self.out_dim, self.atom_size
        )
        value = self.value_layer(val_hid).view(-1, 1, self.atom_size)
        q_atoms = value + advantage - advantage.mean(dim=1, keepdim=True)
        
        dist = F.softmax(q_atoms, dim=-1)
        dist = dist.clamp(min=1e-3)  # for avoiding nans
        
        return dist
    
    def reset_noise(self):
        """Reset all noisy layers."""
        self.advantage_hidden_layer.reset_noise()
        self.advantage_layer.reset_noise()
        self.value_hidden_layer.reset_noise()
        self.value_layer.reset_noise()
```

## 02. Double Q-Learning
- _compute_dqn_loss()é–¢æ•°å†…ã§ã€targetã‚’ä½œæˆã™ã‚‹éš›ã®Qå€¤ã®æ±‚ã‚æ–¹ãŒç•°ãªã‚‹.

##### DQN

```
target = reward + gamma * dqn_target(next_state).max(dim=1, keepdim=True)[0]
```
```
next_q_value = self.dqn_target( # Vanilla DQN
    next_state
).max(dim=1, keepdim=True)[0].detach()
```

##### Double Q-Learning

```
selected_action = dqn(next_state).argmax(dim=1, keepdim=True)
target = reward + gamma * dqn_target(next_state).gather(1, selected_action)
```

```
next_q_value = self.dqn_target(next_state).gather(  # Double DQN
    1, self.dqn(next_state).argmax(dim=1, keepdim=True)
).detach()
```

```
class DQNAgent:
    """DQN Agent interacting with environment.
    
    Attribute:
        env (gym.Env): openAI Gym environment
        memory (PrioritizedReplayBuffer): replay memory to store transitions
        batch_size (int): batch size for sampling
        target_update (int): period for target model's hard update
        gamma (float): discount factor
        dqn (Network): model to train and select actions
        dqn_target (Network): target model to update
        optimizer (torch.optim): optimizer for training dqn
        transition (list): transition information including 
                           state, action, reward, next_state, done
        v_min (float): min value of support
        v_max (float): max value of support
        atom_size (int): the unit number of support
        support (torch.Tensor): support for categorical dqn
        use_n_step (bool): whether to use n_step memory
        n_step (int): step number to calculate n-step td error
        memory_n (ReplayBuffer): n-step replay buffer
    """

    def __init__(
        self, 
        env: gym.Env,
        memory_size: int,
        batch_size: int,
        target_update: int,
        gamma: float = 0.99,
        # PER parameters
        alpha: float = 0.2,
        beta: float = 0.6,
        prior_eps: float = 1e-6,
        # Categorical DQN parameters
        v_min: float = 0.0,
        v_max: float = 200.0,
        atom_size: int = 51,
        # N-step Learning
        n_step: int = 3,
    ):
        """Initialization.
        
        Args:
            env (gym.Env): openAI Gym environment
            memory_size (int): length of memory
            batch_size (int): batch size for sampling
            target_update (int): period for target model's hard update
            lr (float): learning rate
            gamma (float): discount factor
            alpha (float): determines how much prioritization is used
            beta (float): determines how much importance sampling is used
            prior_eps (float): guarantees every transition can be sampled
            v_min (float): min value of support
            v_max (float): max value of support
            atom_size (int): the unit number of support
            n_step (int): step number to calculate n-step td error
        """
        obs_dim = env.observation_space.shape[0]
        action_dim = env.action_space.n
        
        self.env = env
        self.batch_size = batch_size
        self.target_update = target_update
        self.gamma = gamma
        # NoisyNet: All attributes related to epsilon are removed
        
        # device: cpu / gpu
        self.device = torch.device(
            "cuda" if torch.cuda.is_available() else "cpu"
        )
        print(self.device)


        # 1-step learningã¨N-step learningã‚’1å¯¾1ã§ãƒ–ãƒ¬ãƒ³ãƒ‰ã™ã‚‹ã€‚ 
        # ã‹ã¤1-step learningã§ã¯ã€03. Prioritized Replay Buffer (PER)ã¨è¨€ã†æ‰‹æ³•ã‚’æ¡ç”¨ã™ã‚‹ã€‚
        # PER
        # memory for 1-step Learning
        self.beta = beta
        self.prior_eps = prior_eps
        self.memory = PrioritizedReplayBuffer(
            obs_dim, memory_size, batch_size, alpha=alpha
        )
        
        # memory for N-step Learning
        self.use_n_step = True if n_step > 1 else False
        if self.use_n_step:
            self.n_step = n_step
            self.memory_n = ReplayBuffer(
                obs_dim, memory_size, batch_size, n_step=n_step, gamma=gamma
            )
            
        # Categorical DQN parameters
        self.v_min = v_min
        self.v_max = v_max
        self.atom_size = atom_size
        self.support = torch.linspace(
            self.v_min, self.v_max, self.atom_size
        ).to(self.device)

        # networks: dqn, dqn_target
        self.dqn = Network(
            obs_dim, action_dim, self.atom_size, self.support
        ).to(self.device)
        self.dqn_target = Network(
            obs_dim, action_dim, self.atom_size, self.support
        ).to(self.device)
        self.dqn_target.load_state_dict(self.dqn.state_dict())
        self.dqn_target.eval()
        
        # optimizer
        self.optimizer = optim.Adam(self.dqn.parameters())

        # transition to store in memory
        self.transition = list()
        
        # mode: train / test
        self.is_test = False

    def select_action(self, state: np.ndarray) -> np.ndarray:
        """Select an action from the input state."""
        # NoisyNet: no epsilon greedy action selection
        selected_action = self.dqn(
            torch.FloatTensor(state).to(self.device)
        ).argmax()
        selected_action = selected_action.detach().cpu().numpy()
        
        if not self.is_test:
            self.transition = [state, selected_action]
        
        return selected_action

    def step(self, action: np.ndarray) -> Tuple[np.ndarray, np.float64, bool]:
        """Take an action and return the response of the env."""
        next_state, reward, done, _ = self.env.step(action)

        if not self.is_test:
            self.transition += [reward, next_state, done]
            
            # N-step transition
            if self.use_n_step:
                one_step_transition = self.memory_n.store(*self.transition)
            # 1-step transition
            else:
                one_step_transition = self.transition

            # add a single step transition
            if one_step_transition:
                self.memory.store(*one_step_transition)
    
        return next_state, reward, done

    def update_model(self) -> torch.Tensor:
        """Update the model by gradient descent."""
        # PER needs beta to calculate weights
        samples = self.memory.sample_batch(self.beta)
        weights = torch.FloatTensor(
            samples["weights"].reshape(-1, 1)
        ).to(self.device)
        indices = samples["indices"]
        
        # 1-step Learning loss
        elementwise_loss = self._compute_dqn_loss(samples, self.gamma)
        
        # PER: importance sampling before average
        loss = torch.mean(elementwise_loss * weights)
        
        # N-step Learning loss
        # we are gonna combine 1-step loss and n-step loss so as to
        # prevent high-variance. The original rainbow employs n-step loss only.
        if self.use_n_step:
            gamma = self.gamma ** self.n_step
            samples = self.memory_n.sample_batch_from_idxs(indices)
            elementwise_loss_n_loss = self._compute_dqn_loss(samples, gamma)
            elementwise_loss += elementwise_loss_n_loss
            
            # PER: importance sampling before average
            loss = torch.mean(elementwise_loss * weights)

        self.optimizer.zero_grad()
        loss.backward()
        clip_grad_norm_(self.dqn.parameters(), 10.0)
        self.optimizer.step()
        
        # PER: update priorities
        loss_for_prior = elementwise_loss.detach().cpu().numpy()
        new_priorities = loss_for_prior + self.prior_eps
        self.memory.update_priorities(indices, new_priorities)

        # 1ã‚¹ãƒ†ãƒƒãƒ—ã”ã¨ã«ã€epsilon-greedyã®å½¹ç›®ã‚’æ‹…ã†ãŸã‚ã«ã€noiseã‚’åˆæœŸåŒ–ã—ã¦ã€exploration (æ¢ç´¢)ã‚’è¡Œã†ã€‚
        # NoisyNet: reset noise 
        self.dqn.reset_noise()
        self.dqn_target.reset_noise()

        return loss.item()
        
    def train(self, num_frames: int, plotting_interval: int = 200):
        """Train the agent."""
        self.is_test = False
        
        state = self.env.reset()
        update_cnt = 0
        losses = []
        scores = []
        score = 0

        for frame_idx in range(1, num_frames + 1):
            action = self.select_action(state)
            next_state, reward, done = self.step(action)

            state = next_state
            score += reward
            
            # NoisyNet: removed decrease of epsilon
            
            # PER: increase beta
            fraction = min(frame_idx / num_frames, 1.0)
            self.beta = self.beta + fraction * (1.0 - self.beta)

            # if episode ends
            if done:
                state = self.env.reset()
                scores.append(score)
                score = 0

            # if training is ready
            if len(self.memory) >= self.batch_size:
                loss = self.update_model()
                losses.append(loss)
                update_cnt += 1
                
                # if hard update is needed
                if update_cnt % self.target_update == 0:
                    self._target_hard_update()

            # plotting
            if frame_idx % plotting_interval == 0:
                self._plot(frame_idx, scores, losses)
                
        self.env.close()
                
    def test(self, video_folder: str) -> None:
        """Test the agent."""
        self.is_test = True
        
        # for recording a video
        naive_env = self.env
        self.env = gym.wrappers.RecordVideo(self.env, video_folder=video_folder)
        
        state = self.env.reset()
        done = False
        score = 0
        
        while not done:
            action = self.select_action(state)
            next_state, reward, done = self.step(action)

            state = next_state
            score += reward
        
        print("score: ", score)
        self.env.close()
        
        # reset
        self.env = naive_env

    def _compute_dqn_loss(self, samples: Dict[str, np.ndarray], gamma: float) -> torch.Tensor:
        """Return categorical dqn loss."""
        device = self.device  # for shortening the following lines
        state = torch.FloatTensor(samples["obs"]).to(device)
        next_state = torch.FloatTensor(samples["next_obs"]).to(device)
        action = torch.LongTensor(samples["acts"]).to(device)
        reward = torch.FloatTensor(samples["rews"].reshape(-1, 1)).to(device)
        done = torch.FloatTensor(samples["done"].reshape(-1, 1)).to(device)
        
        # Categorical DQN algorithm
        delta_z = float(self.v_max - self.v_min) / (self.atom_size - 1)

        with torch.no_grad():
            # Double DQN
            next_action = self.dqn(next_state).argmax(1)
            next_dist = self.dqn_target.dist(next_state)
            next_dist = next_dist[range(self.batch_size), next_action]

            t_z = reward + (1 - done) * gamma * self.support
            # 06. Categorical DQN
            t_z = t_z.clamp(min=self.v_min, max=self.v_max)
            b = (t_z - self.v_min) / delta_z
            l = b.floor().long()
            u = b.ceil().long()

            offset = (
                torch.linspace(
                    0, (self.batch_size - 1) * self.atom_size, self.batch_size
                ).long()
                .unsqueeze(1)
                .expand(self.batch_size, self.atom_size)
                .to(self.device)
            )

            proj_dist = torch.zeros(next_dist.size(), device=self.device)
            proj_dist.view(-1).index_add_(
                0, (l + offset).view(-1), (next_dist * (u.float() - b)).view(-1)
            )
            proj_dist.view(-1).index_add_(
                0, (u + offset).view(-1), (next_dist * (b - l.float())).view(-1)
            )

        dist = self.dqn.dist(state)
        log_p = torch.log(dist[range(self.batch_size), action])
        elementwise_loss = -(proj_dist * log_p).sum(1)

        return elementwise_loss

    def _target_hard_update(self):
        """Hard update: target <- local."""
        self.dqn_target.load_state_dict(self.dqn.state_dict())
                
    def _plot(
        self, 
        frame_idx: int, 
        scores: List[float], 
        losses: List[float],
    ):
        """Plot the training progresses."""
        clear_output(True)
        plt.figure(figsize=(20, 5))
        plt.subplot(131)
        plt.title('frame %s. score: %s' % (frame_idx, np.mean(scores[-10:])))
        plt.plot(scores)
        plt.subplot(132)
        plt.title('loss')
        plt.plot(losses)
        plt.show()
```

## å­¦ç¿’

```
# environment
env_id = "CartPole-v0"
env = gym.make(env_id)
if IN_COLAB:
    env = gym.wrappers.Monitor(env, "videos", force=True)
seed = 777

def seed_torch(seed):
    torch.manual_seed(seed)
    if torch.backends.cudnn.enabled:
        torch.backends.cudnn.benchmark = False
        torch.backends.cudnn.deterministic = True

np.random.seed(seed)
random.seed(seed)
seed_torch(seed)
env.seed(seed)

# parameters
num_frames = 20000
memory_size = 10000
batch_size = 128
target_update = 100

# train
agent = DQNAgent(env, memory_size, batch_size, target_update)
agent.train(num_frames)
```

## ãƒ†ã‚¹ãƒˆã¨å¯è¦–åŒ–

```
video_folder="videos/rainbow"
agent.test(video_folder=video_folder)

import base64
import glob
import io
import os

from IPython.display import HTML, display


def ipython_show_video(path: str) -> None:
    """Show a video at `path` within IPython Notebook."""
    if not os.path.isfile(path):
        raise NameError("Cannot access: {}".format(path))

    video = io.open(path, "r+b").read()
    encoded = base64.b64encode(video)

    display(HTML(
        data="""
        <video width="320" height="240" alt="test" controls>
        <source src="data:video/mp4;base64,{0}" type="video/mp4"/>
        </video>
        """.format(encoded.decode("ascii"))
    ))


def show_latest_video(video_folder: str) -> str:
    """Show the most recently recorded video from video folder."""
    list_of_files = glob.glob(os.path.join(video_folder, "*.mp4"))
    latest_file = max(list_of_files, key=os.path.getctime)
    ipython_show_video(latest_file)
    return latest_file


latest_file = show_latest_video(video_folder=video_folder)
print("Played:", latest_file)
```


# Follow-up reads

Note that the DDPM paper showed that diffusion models are a promising direction for (un)conditional image generation. This has since then (immensely) been improved, most notably for text-conditional image generation. Below, we list some important (but far from exhaustive) follow-up works:

- Improved Denoising Diffusion Probabilistic Models ([Nichol et al., 2021](https://arxiv.org/abs/2102.09672)): finds that learning the variance of the conditional distribution (besides the mean) helps in improving performance
- Cascaded Diffusion Models for High Fidelity Image Generation ([Ho et al., 2021](https://arxiv.org/abs/2106.15282)): introduces cascaded diffusion, which comprises a pipeline of multiple diffusion models that generate images of increasing resolution for high-fidelity image synthesis
- Diffusion Models Beat GANs on Image Synthesis ([Dhariwal et al., 2021](https://arxiv.org/abs/2105.05233)): show that diffusion models can achieve image sample quality superior to the current state-of-the-art generative models by improving the U-Net architecture, as well as introducing classifier guidance
- Classifier-Free Diffusion Guidance ([Ho et al., 2021](https://openreview.net/pdf?id=qw8AKxfYbI)): shows that you don't need a classifier for guiding a diffusion model by jointly training a conditional and an unconditional diffusion model with a single neural network
- Hierarchical Text-Conditional Image Generation with CLIP Latents (DALL-E 2) ([Ramesh et al., 2022](https://cdn.openai.com/papers/dall-e-2.pdf)): uses a prior to turn a text caption into a CLIP image embedding, after which a diffusion model decodes it into an image
- Photorealistic Text-to-Image Diffusion Models with Deep Language Understanding (ImageGen) ([Saharia et al., 2022](https://arxiv.org/abs/2205.11487)): shows that combining a large pre-trained language model (e.g. T5) with cascaded diffusion works well for text-to-image synthesis

Note that this list only includes important works until the time of writing, which is June 7th, 2022.

For now, it seems that the main (perhaps only) disadvantage of diffusion models is that they require multiple forward passes to generate an image (which is not the case for generative models like GANs). However, there's [research going on](https://arxiv.org/abs/2204.13902) that enables high-fidelity generation in as few as 10 denoising steps.

